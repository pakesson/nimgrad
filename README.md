# Nimgrad

A Nim implementation of Andrej Karpathy's [micrograd](https://github.com/karpathy/micrograd)
engine.

Nimgrad implements backpropagation (reverse-mode automatic differentiation)
with a dynamic computational graph for scalar values.

## Example: Moons

```
$ nim c -d:release -r examples/example_moons_mlp.nim
Number of parameters: 337
Train split size: 70
Test split size: 30
Loss: 1.529085309122311
Accuracy: 0.2
Step 1, loss 1.529085309122311, accuracy 20.0000%
Step 2, loss 1.114062834353085, accuracy 57.1429%
Step 3, loss 0.5964037171656581, accuracy 75.7143%
Step 4, loss 0.9422943400691633, accuracy 77.1429%
Step 5, loss 0.4444780618599209, accuracy 81.4286%
Step 6, loss 0.3721652314937884, accuracy 82.8571%
Step 7, loss 0.3334193877314119, accuracy 84.2857%
Step 8, loss 0.3348949790792737, accuracy 87.1429%
Step 9, loss 0.3802738109702992, accuracy 85.7143%
Step 10, loss 0.3241596015933875, accuracy 88.5714%
Step 11, loss 0.3317501756060175, accuracy 85.7143%
Step 12, loss 0.3129715817201137, accuracy 90.0000%
Step 13, loss 0.3332609415978582, accuracy 85.7143%
Step 14, loss 0.32541766202172, accuracy 85.7143%
Step 15, loss 0.3388549238249075, accuracy 85.7143%
Step 16, loss 0.292030120983699, accuracy 87.1429%
Step 17, loss 0.2984849378749921, accuracy 85.7143%
Step 18, loss 0.2703409061735655, accuracy 87.1429%
Step 19, loss 0.2759714850151737, accuracy 85.7143%
Step 20, loss 0.2640942850536951, accuracy 87.1429%
Step 21, loss 0.2722114669444243, accuracy 85.7143%
Step 22, loss 0.2678677692556809, accuracy 88.5714%
Step 23, loss 0.2981409938636967, accuracy 87.1429%
Step 24, loss 0.2644062826648255, accuracy 90.0000%
Step 25, loss 0.2726247661790112, accuracy 85.7143%
Step 26, loss 0.2383144841339891, accuracy 87.1429%
Step 27, loss 0.2309127868244507, accuracy 87.1429%
Step 28, loss 0.2450074963668107, accuracy 90.0000%
Step 29, loss 0.2503918382235921, accuracy 87.1429%
Step 30, loss 0.2229761560562648, accuracy 90.0000%
Step 31, loss 0.2241327213474543, accuracy 87.1429%
Step 32, loss 0.1938956117171597, accuracy 91.4286%
Step 33, loss 0.1917936496092754, accuracy 91.4286%
Step 34, loss 0.1926767669153791, accuracy 91.4286%
Step 35, loss 0.1900058814699267, accuracy 88.5714%
Step 36, loss 0.1779624577747682, accuracy 91.4286%
Step 37, loss 0.2005366893062052, accuracy 91.4286%
Step 38, loss 0.1794444404116692, accuracy 91.4286%
Step 39, loss 0.1599186928660875, accuracy 94.2857%
Step 40, loss 0.1453118555990052, accuracy 94.2857%
Step 41, loss 0.1401067920166685, accuracy 94.2857%
Step 42, loss 0.1549790356981415, accuracy 95.7143%
Step 43, loss 0.174763044810312, accuracy 91.4286%
Step 44, loss 0.1412978644301757, accuracy 94.2857%
Step 45, loss 0.1208444805378397, accuracy 95.7143%
Step 46, loss 0.1094025391233937, accuracy 98.5714%
Step 47, loss 0.09950994208098736, accuracy 98.5714%
Step 48, loss 0.09079287324804063, accuracy 98.5714%
Step 49, loss 0.08027232840682032, accuracy 98.5714%
Step 50, loss 0.07402139056187974, accuracy 100.0000%
Step 51, loss 0.09718240551150632, accuracy 98.5714%
Step 52, loss 0.115515083877366, accuracy 95.7143%
Step 53, loss 0.08299740560634099, accuracy 95.7143%
Step 54, loss 0.05761688070079913, accuracy 100.0000%
Step 55, loss 0.05144946438713414, accuracy 100.0000%
Step 56, loss 0.04053810530283923, accuracy 100.0000%
Step 57, loss 0.08958659928540706, accuracy 97.1429%
Step 58, loss 0.09638843875308954, accuracy 95.7143%
Step 59, loss 0.03289595908874909, accuracy 100.0000%
Step 60, loss 0.0293051280160633, accuracy 100.0000%
Step 61, loss 0.0303879755445536, accuracy 100.0000%
Step 62, loss 0.03276280751315476, accuracy 100.0000%
Step 63, loss 0.0266638777828573, accuracy 100.0000%
Step 64, loss 0.03707801871910539, accuracy 100.0000%
Step 65, loss 0.0230033115805073, accuracy 100.0000%
Step 66, loss 0.01874816836829527, accuracy 100.0000%
Step 67, loss 0.0158460718454543, accuracy 100.0000%
Step 68, loss 0.02334241953635549, accuracy 100.0000%
Step 69, loss 0.01518781316107912, accuracy 100.0000%
Step 70, loss 0.01347666760328585, accuracy 100.0000%
Step 71, loss 0.01921988964899213, accuracy 100.0000%
Step 72, loss 0.01174285033683488, accuracy 100.0000%
Step 73, loss 0.01467891148507587, accuracy 100.0000%
Step 74, loss 0.01676907617694114, accuracy 100.0000%
Step 75, loss 0.01179948201601297, accuracy 100.0000%
Step 76, loss 0.01189511766602064, accuracy 100.0000%
Step 77, loss 0.01341298825113263, accuracy 100.0000%
Step 78, loss 0.013325907757608, accuracy 100.0000%
Step 79, loss 0.01104214491154475, accuracy 100.0000%
Step 80, loss 0.01359351776731416, accuracy 100.0000%
Step 81, loss 0.01096852614609809, accuracy 100.0000%
Step 82, loss 0.01096733719008541, accuracy 100.0000%
Step 83, loss 0.01096618784326158, accuracy 100.0000%
Step 84, loss 0.01096507809312922, accuracy 100.0000%
Step 85, loss 0.01096400792762002, accuracy 100.0000%
Step 86, loss 0.01096297733509431, accuracy 100.0000%
Step 87, loss 0.01096198630434101, accuracy 100.0000%
Step 88, loss 0.01096103482457736, accuracy 100.0000%
Step 89, loss 0.01096012288544869, accuracy 100.0000%
Step 90, loss 0.01095925047702828, accuracy 100.0000%
Step 91, loss 0.01095841758981718, accuracy 100.0000%
Step 92, loss 0.01095762421474403, accuracy 100.0000%
Step 93, loss 0.01095687034316485, accuracy 100.0000%
Step 94, loss 0.01095615596686302, accuracy 100.0000%
Step 95, loss 0.0109554810780489, accuracy 100.0000%
Step 96, loss 0.01095484566935993, accuracy 100.0000%
Step 97, loss 0.01095424973386036, accuracy 100.0000%
Step 98, loss 0.01095369326504112, accuracy 100.0000%
Step 99, loss 0.01095317625681978, accuracy 100.0000%
Step 100, loss 0.01095269870354037, accuracy 100.0000%
Test split:
Loss: 0.03048524345869338
Accuracy: 100.0000%
```